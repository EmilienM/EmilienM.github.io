[{"content":"This is my second post of a series which will cover how you can distribute your OpenShift cluster across multiple datacenter domains and increase availability and performance of your control plane.\nBackground If you haven\u0026rsquo;t read it, please have a look at the first post.\nFailure Domains Failure Domains help to spread the OpenShift control plane across multiple (at least 3) domains where each domain has a defined storage / network / compute configuration. In a modern datacenter, each domain has its own power unit, network and storage fabric, etc. If a domain goes down, it wouldn\u0026rsquo;t have an impact on the workloads since the other domains are healthy and the services are deployed in HA.\nIn this context, we think that the SLA of OpenShift can significantly be increased by deploying the OpenShift cluster (control plane and workloads) across at least 3 domains.\nIn OCP 4.13, Failure Domains will be TechPreview (not supported) but you can still test it. We plan to make it supported in a future release.\nIf you remember the previous post, we were deploying OpenShift within one domain, with one external load balancer. Now that we have Failure Domains, let\u0026rsquo;s deploy 3 external LBs (one in each domain) and then a cluster that is distributed over 3 domains.\nPre-requisites At least 3 networks and subnets (can be tenant or provider networks) have to be pre-created. They need to be reachable from where Ansible will be run. The machines used for the LB have to be deployed on CentOS9 (this is what we test).\nDeploy your own Load-Balancers In our example, we\u0026rsquo;ll deploy one LB per leaf, which is in its own routed network. Therefore, we\u0026rsquo;ll deploy 3 load balancers.\nLet\u0026rsquo;s deploy!\nCreate your Ansible inventory.yaml file:\n--- all: hosts: lb1: ansible_host: 192.168.11.2 lb2 ansible_host: 192.168.12.2 lb3: ansible_host: 192.168.13.2 vars: ansible_user: cloud-user ansible_become: true config: lb Create the Ansible playbook.yaml file:\n--- - hosts: - lb1 - lb2 - lb3 tasks: - name: Deploy the LBs include_role: name: emilienm.routed_lb Write the LB configs in Ansible vars.yaml:\n--- configs: lb: bgp_asn: 64998 bgp_neighbors: - ip: 192.168.11.1 password: f00barZ services: - name: api vips: - 192.168.100.240 min_backends: 1 healthcheck: \u0026#34;httpchk GET /readyz HTTP/1.0\u0026#34; balance: roundrobin frontend_port: 6443 haproxy_monitor_port: 8081 backend_opts: \u0026#34;check check-ssl inter 1s fall 2 rise 3 verify none\u0026#34; backend_port: 6443 backend_hosts: \u0026amp;lb_hosts - name: rack1-10 ip: 192.168.11.10 - name: rack1-11 ip: 192.168.11.11 - name: rack1-12 ip: 192.168.11.12 - name: rack1-13 ip: 192.168.11.13 - name: rack1-14 ip: 192.168.11.14 - name: rack1-15 ip: 192.168.11.15 - name: rack1-16 ip: 192.168.11.16 - name: rack1-17 ip: 192.168.11.17 - name: rack1-18 ip: 192.168.11.18 - name: rack1-19 ip: 192.168.11.19 - name: rack1-20 ip: 192.168.11.20 - name: rack2-10 ip: 192.168.12.10 - name: rack2-11 ip: 192.168.12.11 - name: rack2-12 ip: 192.168.12.12 - name: rack2-13 ip: 192.168.12.13 - name: rack2-14 ip: 192.168.12.14 - name: rack2-15 ip: 192.168.12.15 - name: rack2-16 ip: 192.168.12.16 - name: rack2-17 ip: 192.168.12.17 - name: rack2-18 ip: 192.168.12.18 - name: rack2-19 ip: 192.168.12.19 - name: rack2-20 ip: 192.168.12.20 - name: rack3-10 ip: 192.168.13.10 - name: rack3-11 ip: 192.168.13.11 - name: rack3-12 ip: 192.168.13.12 - name: rack3-13 ip: 192.168.13.13 - name: rack3-14 ip: 192.168.13.14 - name: rack3-15 ip: 192.168.13.15 - name: rack3-16 ip: 192.168.13.16 - name: rack3-17 ip: 192.168.13.17 - name: rack3-18 ip: 192.168.13.18 - name: rack3-19 ip: 192.168.13.19 - name: rack3-20 ip: 192.168.13.20 - name: ingress_http vips: - 192.168.100.250 min_backends: 1 healthcheck: \u0026#34;httpchk GET /healthz/ready HTTP/1.0\u0026#34; frontend_port: 80 haproxy_monitor_port: 8082 balance: roundrobin backend_opts: \u0026#34;check check-ssl port 1936 inter 1s fall 2 rise 3 verify none\u0026#34; backend_port: 80 backend_hosts: *lb_hosts - name: ingress_https vips: - 192.168.100.250 min_backends: 1 healthcheck: \u0026#34;httpchk GET /healthz/ready HTTP/1.0\u0026#34; frontend_port: 443 haproxy_monitor_port: 8083 balance: roundrobin backend_opts: \u0026#34;check check-ssl port 1936 inter 1s fall 2 rise 3 verify none\u0026#34; backend_port: 443 backend_hosts: *lb_hosts - name: mcs vips: - 192.168.100.240 min_backends: 1 frontend_port: 22623 haproxy_monitor_port: 8084 balance: roundrobin backend_opts: \u0026#34;check check-ssl inter 5s fall 2 rise 3 verify none\u0026#34; backend_port: 22623 backend_hosts: *lb_hosts In this case, we deploy OpenShift on OpenStack which doesn\u0026rsquo;t support static IPs. Therefore, we have to put all the available IPs from the subnets used for the machines, in the HAproxy backends.\nInstall the role and the dependencies:\nansible-galaxy install emilienm.routed_lb,1.0.0 ansible-galaxy collection install ansible.posix ansible.utils Deploy the LBs:\nansible-playbook -i inventory.yaml -e \u0026#34;@vars.yaml\u0026#34; playbook.yaml Deploy OpenShift Here is an example of install-config.yaml:\napiVersion: v1 baseDomain: mydomain.test compute: - name: worker platform: openstack: type: m1.xlarge replicas: 1 controlPlane: name: master platform: openstack: type: m1.xlarge failureDomains: - portTargets: - id: control-plane network: id: fb6f8fea-5063-4053-81b3-6628125ed598 fixedIPs: - subnet: id: b02175dd-95c6-4025-8ff3-6cf6797e5f86 - portTargets: - id: control-plane network: id: 9a5452a8-41d9-474c-813f-59b6c34194b6 fixedIPs: - subnet: id: 5fe5b54a-217c-439d-b8eb-441a03f7636d - portTargets: - id: control-plane network: id: 3ed980a6-6f8e-42d3-8500-15f18998c434 fixedIPs: - subnet: id: a7d57db6-f896-475f-bdca-c3464933ec02 replicas: 3 metadata: name: mycluster networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.11.0/24 - cidr: 192.168.100.0/24 platform: openstack: cloud: mycloud machinesSubnet: 8586bf1a-cc3c-4d40-bdf6-c243decc603a apiVIPs: - 192.168.100.240 ingressVIPs: - 192.168.100.250 loadBalancer: type: UserManaged featureSet: TechPreviewNoUpgrade Note that for each Failure Domain, you have to provide the leaf network ID and its subnet ID as well. If you deploy with availability zones, you\u0026rsquo;ll be able to provide them in each domain. The documentation for this feature is in progress and I\u0026rsquo;ll update this post once we have it published.\nAlso, please note that I\u0026rsquo;m currently working on a demo and I\u0026rsquo;ll publish the link in this article when it\u0026rsquo;s done.\nKnown limitations Deploying OpenShift with static IPs for the machines is not supported with OpenStack platform for now. Changing the IP address for any OpenShift control plane VIP (API + Ingress) is currently not supported. So once the external LB and the OpenShift cluster is deployed, the VIPs can\u0026rsquo;t be changed. Migrating an OpenShift cluster from the OpenShift managed LB to an external LB is currently not supported. Failure Domains are only for the control plane for now, and will be extended to the compute nodes. Keep in mind that the features will be TechPreview at first and once it has reached some maturity, we\u0026rsquo;ll promote them to GA.\nWrap-up In this article, we combined both exciting features that will help to increase your SLA and also improve the performances not only on the control plane but also for the workloads.\nWe have already got positive feedback from various teams, who tested it at a large scale and demonstrated that in this scenario, OpenShift is more reliable, better load-balanced and distributed in case of failure.\nI hope you liked it and please provide any feedback on the channels.\n","permalink":"https://my1.fr/blog/openshift-external-load-balancer-control-plane-with-failure-domains/","summary":"\u003cp\u003eThis is my second post of a series which will cover how you can distribute your OpenShift cluster across multiple datacenter domains and increase\navailability and performance of your control plane.\u003c/p\u003e","title":"Deploying OpenShift on OpenStack with an External Load-Balancer for your control plane in multiple Failure Domains"},{"content":"This is my first post of a series which will cover how you can distribute your OpenShift cluster across multiple datacenter domains and increase availability and performance of your control plane.\nBackground Originally the on-premise OpenShift IPI architecture was designed to deploy an internal (called OpenShift Managed) load balancer based on HAproxy and Keepalived. However when you want to distribute your cluster across multiple failure domains, your control plane has to be deployable on multiple L2 networks, which are usually isolated per rack, and routed with protocols like BGP.\nStretched vs L3 networks A single stretched L2 network brings challenges:\nNetwork latency is not predictable Traffic bottlenecks L2 domain failures Network management complexity Smaller (L3 routed) networks however has these benefits:\nOptimize East-West traffic Low and predictable latency Easier to extend and manage Failure domain isolated to a network Non blocking network fabric OpenShift Managed Load-Balancer For on-prem platforms (VSphere, Baremetal, OpenStack, Ovirt and Nutanix), the control plane load balancer is based on HAproxy and Keepalived. It means that the control plane VIPs (for API \u0026amp; Ingress services) will be managed in Active/Passive mode. The Keepalived master (elected by VRRPv2) will host the VIPs and therefore all the API \u0026amp; Ingress traffic will always go through one node and then load-balanced across the control plane. This bottleneck has been an issue at large scale.\nAlso, Keepalived doesn\u0026rsquo;t deal with L3 routing, so if the VIPs aren\u0026rsquo;t within the same subnet as the L2 networks, the network fabric can\u0026rsquo;t know where the VIPs actually are.\nUser Managed Load-Balancer When we initially looked at the limitations of the OpenShift Managed Load-Balancer, we thought we would just add BGP to the OpenShift control plane, so the VIPs could be routed across the datacenter. You can have a look at this demo that shows how it would work. After the initial proposal which brought up a lot of good ideas, it was decided that for now we would rather try to externalize the Load-Balancer and let the customers dealing with it, rather than implementing something new in OpenShift (I\u0026rsquo;ll come back to it the wrap-up).\nIndeed, a lot of our customers already have (enterprise-grade) load balancers that they use for their workloads. Some of them want to re-use these appliances and manage the OpenShift control plane traffic with them.\nWe realized that some of them want BGP, some of them don\u0026rsquo;t. Some want to keep stretched L2 networks, some don\u0026rsquo;t. There were a lot of decisions we would make if we would have implemented BGP within the OpenShift control plane so we decided that for now we will allow to use an external (user-managed) load balancer, like it\u0026rsquo;s already the case for the workloads themselves (e.g. with MetalLB).\nMore details on the design can be found in this OpenShift enhancement.\nDeploy your own Load-Balancer I want to share how someone can deploy a load balancer that will be used by the OpenShift control plane. For that, I\u0026rsquo;ve decided to create an Ansible role named ansible-role-routed-lb.\nThis will deploy an advanced Load-Balancer capable of managing routed VIPs with FRR (using BGP) and load-balance traffic with HAproxy.\nThe role will do the following:\nIf BGP neighbors are provided in the config, it\u0026rsquo;ll deploy FRR and peer with your BGP neighbor(s). If the VIPs are created on the node, they\u0026rsquo;ll be routed in your infrastructure. Deploy HAproxy to load-balance and monitor your service. If the VIPs are provided in the config, they will be created if a minimum number of backend(s) are found healthy for a given service, and therefore routed in BGP if FRR is deployed. They will be removed if no backend was found healthy for a given service, therefore not routed in BGP if FRR is deployed So if you\u0026rsquo;re hosting multiple Load-Balancers, your OpenShift control plane traffic will be:\nrouted thanks to BGP if FRR is deployed load-balanced and high-availability at the VIP level thanks to BGP if FRR is deployed load-balanced between healthy backends thanks to HAproxy Let\u0026rsquo;s deploy it!\nIn this blog post, we won\u0026rsquo;t cover the Failure Domains yet, and will deploy OpenShift within a single Leaf. Therefore, we\u0026rsquo;ll deploy only one load balancer.\nCreate your Ansible inventory.yaml file:\n--- all: hosts: lb: ansible_host: 192.168.11.2 ansible_user: cloud-user ansible_become: true 192.168.11.2 is the IP address of the load balancer.\nCreate the Ansible playbook.yaml file:\n--- - hosts: lb vars: config: lb tasks: - name: Deploy the LBs include_role: name: emilienm.routed_lb Write the LB configs in Ansible vars.yaml:\n--- configs: lb: bgp_asn: 64998 bgp_neighbors: - ip: 192.168.11.1 password: f00barZ services: - name: api vips: - 192.168.100.240 min_backends: 1 healthcheck: \u0026#34;httpchk GET /readyz HTTP/1.0\u0026#34; balance: roundrobin frontend_port: 6443 haproxy_monitor_port: 8081 backend_opts: \u0026#34;check check-ssl inter 1s fall 2 rise 3 verify none\u0026#34; backend_port: 6443 backend_hosts: \u0026amp;lb_hosts - name: rack1-10 ip: 192.168.11.10 - name: rack1-11 ip: 192.168.11.11 - name: rack1-12 ip: 192.168.11.12 - name: rack1-13 ip: 192.168.11.13 - name: rack1-14 ip: 192.168.11.14 - name: rack1-15 ip: 192.168.11.15 - name: rack1-16 ip: 192.168.11.16 - name: rack1-17 ip: 192.168.11.17 - name: rack1-18 ip: 192.168.11.18 - name: rack1-19 ip: 192.168.11.19 - name: rack1-20 ip: 192.168.11.20 - name: ingress_http vips: - 192.168.100.250 min_backends: 1 healthcheck: \u0026#34;httpchk GET /healthz/ready HTTP/1.0\u0026#34; frontend_port: 80 haproxy_monitor_port: 8082 balance: roundrobin backend_opts: \u0026#34;check check-ssl port 1936 inter 1s fall 2 rise 3 verify none\u0026#34; backend_port: 80 backend_hosts: *lb_hosts - name: ingress_https vips: - 192.168.100.250 min_backends: 1 healthcheck: \u0026#34;httpchk GET /healthz/ready HTTP/1.0\u0026#34; frontend_port: 443 haproxy_monitor_port: 8083 balance: roundrobin backend_opts: \u0026#34;check check-ssl port 1936 inter 1s fall 2 rise 3 verify none\u0026#34; backend_port: 443 backend_hosts: *lb_hosts - name: mcs vips: - 192.168.100.240 min_backends: 1 frontend_port: 22623 haproxy_monitor_port: 8084 balance: roundrobin backend_opts: \u0026#34;check check-ssl inter 5s fall 2 rise 3 verify none\u0026#34; backend_port: 22623 backend_hosts: *lb_hosts In this case, we deploy OpenShift on OpenStack which doesn\u0026rsquo;t support static IPs. Therefore, we have to put all the available IPs from the subnet used for the machines, in the HAproxy backends.\nInstall the role and the dependencies:\nansible-galaxy install emilienm.routed_lb,1.0.0 ansible-galaxy collection install ansible.posix ansible.utils Deploy the LBs:\nansible-playbook -i inventory.yaml -e \u0026#34;@vars.yaml\u0026#34; playbook.yaml Deploy OpenShift This feature will be available in the 4.13 release as TechPreview.\nHere is how you can simply enable it via the install-config.yaml:\napiVersion: v1 baseDomain: mydomain.test compute: - name: worker platform: openstack: type: m1.xlarge replicas: 3 controlPlane: name: master platform: openstack: type: m1.xlarge replicas: 3 metadata: name: mycluster networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.11.0/24 - cidr: 192.168.100.0/24 platform: openstack: cloud: mycloud machinesSubnet: 8586bf1a-cc3c-4d40-bdf6-c243decc603a apiVIPs: - 192.168.100.240 ingressVIPs: - 192.168.100.250 loadBalancer: type: UserManaged featureSet: TechPreviewNoUpgrade You can also watch this demo which shows the outcome.\nKnown limitations Deploying OpenShift with static IPs for the machines is only supported on Baremetal platform for now but it\u0026rsquo;s in the roadmap to support it on VSphere and OpenStack as well. Changing the IP address for any OpenShift control plane VIP (API + Ingress) is currently not supported. So once the external LB and the OpenShift cluster is deployed, the VIPs can\u0026rsquo;t be changed. This is in our roadmap. Migrating an OpenShift cluster from the OpenShift managed LB to an external LB is currently not supported. It\u0026rsquo;s in our roadmap as well. Keep in mind that the feature will be TechPreview at first and once it has reached some maturity, we\u0026rsquo;ll promote it to GA.\nWrap-up Having the VIPs highly available, routed across multiple domains is only a first step into distributing the OpenShift control plane. In the future, we\u0026rsquo;ll discuss about how Failure Domains will be configured when deploying OpenShift on OpenStack. Note that this is already doable on Baremetal and Vsphere.\nWith this effort, our customers can now decide which Load-Balancer to use, and if they have some expertise in their appliance now they can use it for the OpenShift control plane.\nThe way we implemented it is flexible and will allow us to implement new load balancers in OpenShift if we want to in the future. The proof of concept done a few months ago with BGP in the control plane could be restored if there is growing interest.\nI hope you liked this article and stay tuned for the next ones!\n","permalink":"https://my1.fr/blog/openshift-external-load-balancer-control-plane-intro/","summary":"\u003cp\u003eThis is my first post of a series which will cover how you can distribute your OpenShift cluster across multiple datacenter domains and increase\navailability and performance of your control plane.\u003c/p\u003e","title":"Deploying OpenShift with an External Load-Balancer for your control plane"},{"content":"Stay tuned on our recent achievements in the Kubernetes and OpenStack space when running Fast-Datapath applications.\nAuthors: Emilien Macchi and Maysa Macedo.\nIn the past months, the Kubernetes Network Plumbing Working-Group added new features to the SR-IOV Network Operator for the OpenStack platform.\nIf you’re not familiar with this operator, it helps Kubernetes cluster users deploy their workloads to be connected to Fast Datapath (FDP) networking resources. While the operator is named “SR-IOV”, we’ll see that it can also manage other types of connectivity.\nIn fact, the operator helps to provision and configure the SR-IOV Network Device Plugin for Kubernetes, which is in charge of discovering and advertising networking resources for FDP, mainly (but not exclusively) for SR-IOV Virtual Functions (VFs) and PCI Physical Functions (PFs) that are available on a Kubernetes host (usually a worker node).\nThe operator hides some complexity to achieve that and provides an easy user interface.\nOpenStack metadata support The operator originally required config-drives to be enabled for the machines connected to the FDP networking, so it could read the OpenStack metadata and Network data.\nWe removed that requirement by adding support for reading that information from the Nova metadata service if no config-drive was used.\nIf your Kubernetes hosts have access to the Nova metadata URL, then you have nothing to do! Otherwise, you’ll need to make sure to create the machines with config-drive enabled.\nEnable VFIO with NOIOMMU In virtual deployments of Kubernetes where the underlying virtualization platform (e.g. QEMU) has support for virtualized I/O memory management unit (IOMMU) however OpenStack Nova doesn\u0026rsquo;t know how to handle it yet. It\u0026rsquo;s a work in progress. Therefore, the VFIO PCI driver needs to be loaded with an option named enable_unsafe_noiommu_mode enabled. This option gives user-space I/O access to a device which is direct memory access capable without a IOMMU.\nThe operator is now loading the driver with the right arguments so the users don’t have to worry about it.\nDPDK The operator was initially designed to work on Baremetal and not necessarily on virtualized platforms. However, when a virtualized Kubernetes host is connected to some network hardware using DPDK, the device is exposed as a virtio interface (seen as a VF by the operator) but to take advantage of DPDK, the device has to use the VFIO-PCI driver. We added support for detecting vhost-user interfaces that are connected to the specified Neutron network used for DPDK. Vhost-user is a module part of DPDK and it helps to run networking in the user-space. You can find more information here.\nHere is an example of a SriovNetworkNodePolicy that can be used for Intel devices (you’ll need to change a few things if your device is Mellanox):\napiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: dpdk1 namespace: openshift-sriov-network-operator spec: deviceType: vfio-pci # change to netdevice if Mellanox nicSelector: netFilter: openstack/NetworkID:55a54d05-9ec1-4051-8adb-1b5a7be4f1b6 nodeSelector: feature.node.kubernetes.io/network-sriov.capable: \u0026#39;true\u0026#39; numVfs: 1 priority: 99 resourceName: dpdk1 isRdma: false # set to true if Mellanox You’ll need to configure the Network ID that matches your DPDK network in OpenStack.\nOVS Hardware Offload Open-vSwitch is CPU intensive, which affects system performance and prevents available bandwidth from being fully utilized.\nSince OVS 2.8 a feature called OVS Hardware Offload is available. It improves performance significantly by offloading tasks to the hardware running the NIC. OpenStack has full compatibility with this feature and the SR-IOV operator can now take advantage of it.\nHere is an example of a SriovNetworkNodePolicy that can be used:\napiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: hwoffload1 namespace: openshift-sriov-network-operator spec: deviceType: netdevice nicSelector: netFilter: openstack/NetworkID:55a54d05-9ec1-4051-8adb-1b5a7be4f1b6 nodeSelector: feature.node.kubernetes.io/network-sriov.capable: \u0026#39;true\u0026#39; numVfs: 1 priority: 99 resourceName: hwoffload1 isRdma: true For now, we only support certain types of devices from the Mellanox vendor.\nAlso, you’ll need to configure the Network ID that matches your offloaded network for OpenStack.\nWrap-up The SR-IOV Network operator was extended to support essential use-cases for OpenStack, so the workloads can be using FDP features. All the features are available in the upstream operator. If you’re an OpenShift user, it’ll be available to you in the 4.11 release and backported to 4.10 in the next zstream, so stay tuned!\n","permalink":"https://my1.fr/blog/sriov-network-operator-improvements-openstack/","summary":"\u003cp\u003eStay tuned on our recent achievements in the Kubernetes and OpenStack space when running Fast-Datapath applications.\u003c/p\u003e","title":"SR-IOV network operator improvements for OpenStack"},{"content":"Read this post to learn more how to update a container in TripleO on a live system.\nNote: this might sound surgery but this is I think the clean options to patch container images in TripleO.\nYour TripleO cloud is running and you want to update an rpm in one or multiple containers?\nTripleO provides some CLI to build new container images with the rpms that you want. This procedure is also documented here.\nIn this particular example, we will update the python3-networking-ovn rpm on octavia_api.\nYou need a host to build the image: The easiest place is the Undercloud or the Standalone node, where Buildah and tripleoclient are installed. We\u0026rsquo;ll build the image from that host.\nPut your rpms in a directory: e.g. in /tmp/rpms\nExport the OpenStack admin credentials: e.g.: export OS_CLOUD=standalone Login to the registry (when using OSP): podman login registry.redhat.io Build the new container image for octavia_api: openstack tripleo container image hotfix \\ --image registry.redhat.io/rhosp-rhel8/openstack-octavia-api:16.2 \\ --rpms-path /tmp/hotfix \\ --tag 16.2-customfix \\ You should see the new image by running buildah images.\nNow you\u0026rsquo;ll need to push the image to a registry (yours, or TripleO registry): e.g. :\nbuildah push registry.redhat.io/rhosp-rhel8/openstack-octavia-api:16.2-16.2-customfix docker://quay.io/emilien/openstack-octavia-api:16.2-customfix Now, there are two methods for deploying that new image.\nRun the deploy command again, after updating the ContainerOctaviaApiImage parameter in TripleO environment Run the following steps: You need to figure out what\u0026rsquo;s the TripleO step where Octavia is deployed (it\u0026rsquo;s step 4), by looking on the host in /var/lib/tripleo-config/container-startup-config and grep for octavia_api.\nNow, go on the host where you want to use that new image (in the case of Standalone, it\u0026rsquo;s the same host where you built the image) and create an Ansible playbook with this content (e.g. paunch.yaml):\n- hosts: localhost become: true vars: service_name: octavia_api tasks: - name: Stop and clean the old container command: systemctl stop {{ service name }} \u0026amp;\u0026amp; podman rm {{ service_name }} - name: Start containers for step 1 paunch: config: /var/lib/tripleo-config/container-startup-config/step_4/hashed-{{ service_name }}.json config_overrides: octavia_api: image: quay.io/emilien/openstack-octavia-api:16.2-customfix\u0026#34; config_id: tripleo_step4 cleanup: false action: apply Change the content for your needs (different step, image, etc).\nRun Ansible with:\nansible-playbook paunch.yaml Your container is now running with your custom image (check with podman inspect).\nFor more details or help, check out the TripleO manuals or ask for help on IRC #tripleo (OFTC now).\n","permalink":"https://my1.fr/blog/patching-containers-in-tripleo/","summary":"\u003cp\u003eRead this post to learn more how to update a container in TripleO on a live system.\u003c/p\u003e","title":"Patching containers in TripleO"},{"content":"Have a look at how we can move container images from the docker.io registry to quay.io.\nThanks to Skopeo, we can copy container images from one registry to another.\nIn this post, we\u0026rsquo;ll copy images from docker.io to quay.io, a container registry which has a lot of features that docker.io doesn\u0026rsquo;t provide. Two of them that I really like are:\nList and manage image vulnerabilities and other security information Manage the manifests of an image If you want more information, checkout their documentation.\nI wrote a small script that one can use to automate the copy of images.\nBefore running the script:\nGet OAuth token from: https://quay.io/organization/[your-org]?tab=applications Change the token, namespace, containers and tag (if needed) If your docker.io registry requires authentication, you\u0026rsquo;ll need to run podman login docker.io (--src-creds option could also be used with Skopeo) You\u0026rsquo;ll need to authenticate against your quay.io registry with podman login quay.io (--dest-creds option could also be used with Skopeo) #!/bin/sh set -ex # get OAuth token from https://quay.io/organization/[your-org]?tab=applications token=\u0026#39;secrete\u0026#39; namespace=yourorg containers=\u0026#39;app1 app2\u0026#39; tag=latest retry() { local -r -i max_attempts=\u0026#34;$1\u0026#34;; shift local -r cmd=\u0026#34;$@\u0026#34; local -i attempt_num=1 until $cmd do if ((attempt_num==max_attempts)) then echo \u0026#34;Attempt $attempt_num failed and there are no more attempts left!\u0026#34; return 1 else echo \u0026#34;Attempt $attempt_num failed! Trying again in $attempt_num seconds...\u0026#34; sleep $((attempt_num++)) fi done } for container in $containers; do # create empty public repo first otherwise skopeo will create the image as private curl -X POST https://quay.io/api/v1/repository \\ -d \u0026#39;{\u0026#34;namespace\u0026#34;:\u0026#34;\u0026#39;$namespace\u0026#39;\u0026#34;,\u0026#34;repository\u0026#34;:\u0026#34;\u0026#39;$container\u0026#39;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Container image \u0026#39;$container\u0026#39;\u0026#34;,\u0026#34;visibility\u0026#34;:\u0026#34;public\u0026#34;}\u0026#39; \\ -H \u0026#39;Authorization: Bearer \u0026#39;$token\u0026#39;\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; # workaround if quay.io returns 500 error, likely due to an internal bug when using skopeo against docker.io copy=\u0026#34;skopeo copy docker://docker.io/$namespace/$container:$tag docker://quay.io/$namespace/$container:$tag\u0026#34; retry 5 $copy done As you can see, there are 2 unusual things in this script:\nThe curl creates an empty public image otherwise quay.io would create a private image by default when copying the image with Skopeo. As far as I know, there is no option in quay.io to change the default policy. Of course, remove it if you don\u0026rsquo;t want your image to be public by default. The retry mechanism is to workaround the 500 error that you might get when it provisions a new repository, and it says it already exists (sounds specific to how the registry receives authentication from Skopeo vs Docker CLI). Enjoy Skopeo \u0026amp; quay.io!\n","permalink":"https://my1.fr/blog/moving-container-images-from-docker-io-to-quay-io/","summary":"\u003cp\u003eHave a look at how we can move container images from the docker.io registry to quay.io.\u003c/p\u003e","title":"Moving container images from docker.io to quay.io"},{"content":"I finally took some time to write some thoughts about what Leadership means to me.\nDuring April 2017, I’ve been very lucky to attend the Leadership training, organized by Zingtrain, paid by the OpenStack Foundation and sponsored by my employer (Red Hat) who paid for the trip to go in Ann Arbor. Thank you to all of them!. I also would like to thank Colette Alexander who made this happen.\nIn this blog post, I’ll explain what I’ve learned and also what I took away during this training but also my career; also I’ll give some personal opinions that only engage myself and nobody else.\nFour {Levels, Stages} of Learning Being a leader starts with the willingness of learning. Let’s start by the four levels of learning:\nListening, Reflecting, Assimilating and acting, Teaching (Repeat)\nThe things mentioned during the training were very close of how I personally learned how to be an Open-Source contributor. It starts by listening around you. It has been a little bit frustrating for me at the beginning to not being able to quickly take actions when new ideas come up, but being patient is really worth it.\nThe time to reflect is important to assimilate what happens out there: “what people do” and “why”, “how to they work together” and “how my contribution would fit in there” are the biggest questions I ask myself most of the time I’m jumping into something new for me.\nThen it’s time for actions. That time is really interesting because it’s very exiting at the beginning when contributing for the first time to a project, but can also be frustrating when getting the first feedback of this contribution. It’s like an “emotional elevator” where you go from total happiness of finally feeling useful in this project to “I’m so frustrated, the way I proposed my idea was rejected, I just want to thrash everything and run away”. This moment is to me very crucial and usually I manage to get my frustration out by going for a run or do some other activities that I like. Coming back on keyboard, I take time to retrospect and see how can I do better the next time.\nNow you’re part of the project and you know how to contribute, the work is not finished. Quite some times I see some projects where it’s hard to join the team because there is simply nobody willing to take time and explain you the really basics. Note: on the other side, it also comes with the capacity of saying “I don’t know” (yeah it happens, period.) and learners have to be ready to be mentored. Anyway, if you know something, teach it so more people will know it and your project will remain a cool place to work for.\nLet’s talk about the four stages of the learning journey.\nIt starts by being Unconsciously Incompetent. You underestimate the skills required to contribute and you jump into this hole without knowing that it’s not going to be easy. This stage is usually fast when you become consciously incompetent and realize it won’t be so easy. Don’t give up and go learning, you’ll become unconsciously incompetent (when you start to be productive and teach what you’ve learned). And then it comes the time to be consciously competent. If you didn’t start to teach the skill to someone else, it’s never to late to do it. If you want to read my personal experience of being a Project Lead in OpenStack community, I wrote a blog post that mentions these Learning steps.\nThe importance of a vision There are different versions out there of what is a Vision. A Vision is not a Mission Statement nor a Strategic Plan. My definition from what I’ve seen and learned over the last years would be: “a vision tells a successful story about what you want to be and where you want to go”.\nAn effective Vision is:\nwritten collectively (where all individuals part of the story can contribute) inspiring people who work with you but also externals strategically sound documented and communicated It starts by taking your pen and write yourself on paper your first draft. I find it important to highlight “you” and “draft” because to me a good Vision takes time and iterations to be well written by yourself and not by any consultant.\nDuring the training, for the first time I wrote a vision of my life in 1 year and I found the exercise interesting. Also, when I came back I started this work with my team at Red Hat. So far it has been very helpful to document where the team wants to go.\nGood Leaders offer great service to staff A good Leader is not a boss, nor a Chief. A Leader is an human who makes the best as possible to serve a team who work on a common purpose. During my last 5 years, people who inspired me were Leaders in some sorts. They help others to be better, share their knowledge, accept failures and learn from them.\nTo me, a good Leader is someone able to drive a project to success without taking any decision, but instead, influence her / his peers by engaging collaboration to make the work happen.\nSomething we learned during the training: Power = 1 / ( Authority x Frequency of use )\nAlso two things I’ve learned over the last years and also were confirmed during the training:\nMultitasking doesn’t work. Being a Leader doesn’t mean you have to be busier than others so you can do multiple things in the same time. First of all: everyone is busy (period again.); Second: it’s impossible for the most common brains to perform in a successful way multiple tasks at the same time. High performing has nothing to do with skills. It’s a matter of how much your team shares a common understanding at how they can work together for a specific purpose (“It’s easy to do the right thing, but hard to do the bad things”). Working fourteen hours per day is not efficient and knowing everything doesn’t mean you’re a good Leader. Bottom-line change is leadership I’m convinced that it exists multiple methods to be a great leader and bring new ideas. One of them might be the BLC (Bottom-line change). It appears to be useful when you (leader) wants to bring a new idea in your team.\nFirst of all, you need to make sure you’ll have some time to dedicate because people won’t always buy your idea so quickly. You need to prepare your idea: write some background, define a problem to solve, and if possible get some valid data to justify your proposal.\nOne of the key things is to get the right people involved in your idea. If your idea is a new feature, get all stakeholders involved (one person per group is enough), and rewrite the idea with them, so all of them agree on it. This step is very useful so when you’ll present results, people will recognize their interests since you asked to the right people. Engage the microcosm to work on the vision and prepare a plan for the change. Share the results with your team and help them to implement the change by giving support and accept feedback.\nAs a conclusion, I would define Leadership as a skill that you can’t learn only in the books (but some books are very useful like Being a Better Leader). You need to practice, try, fail, retrospect and try again. Being a leader in some tasks is very rewarding and in my opinion sometimes reduces frustration. At least but not least, being a good leader and going the extra-mile can be to create new leaders around you by sharing techniques, trusting and promoting people. Have fun!\n","permalink":"https://my1.fr/blog/what-leadership-means-to-me/","summary":"\u003cp\u003eI finally took some time to write some thoughts about what Leadership means to me.\u003c/p\u003e","title":"What Leadership Means To Me"},{"content":"This story explains why I started to stop working as a anarchistic-multi-tasking-schedule-driven and learnt how to become a good team leader.\nHow it started March 2015, Puppet OpenStack project just moved under the Big Tent. What a success for our group!\nOne of the first step was to elect a Project Team Lead. Our group was pretty small (~10 active contributors) so we thought that the PTL would be just a facilitator for the group, and the liaison with other projects that interact with us. I mean, easy, right?\nAt that time, I was clearly an unconsciously incompetent PTL. I thought I knew what I was doing to drive the project to success.\nBut situation evolved. I started to deal with things that I didn\u0026rsquo;t expect to deal with like making sure our team works together in a way that is efficient and consistent. I also realized nobody knew what a PTL was really supposed to do (at least in our group), so I took care of more tasks, like release management, organizing Summit design sessions, promoting core reviewers, and welcoming newcomers. That was the time where I realized I become a consciously incompetent PTL. I was doing things that nobody taught me before.\nIn fact, there is no book telling you how to lead an OpenStack project so I decided to jump in this black hole and hopefully I would make mistakes so I can learn something.\nSet your own expectations I made the mistake of engaging myself into a role where expectations were not cleared with the team. The PTL guide is not enough to clear expectations of what your team will wait from you. This is something you have to figure out with the folks you\u0026rsquo;re working with. You would be surprised by the diversity of expectations that project contributors have for their PTL. Talk with your team and ask them what they want you to be and how they see you as a team lead. I don\u0026rsquo;t think there is a single rule that works for all projects, because of the different cultures in OpenStack community.\nEmbrace changes … and accept failures. There is no project in OpenStack that didn\u0026rsquo;t had outstanding issues (technical and human). The first step as a PTL is to acknowledge the problem and share it with your team. Most of the conflicts are self-resolved when everyone agrees that yes, there is a problem. It can be a code design issue or any other technical disagreement but also human complains, like the difficulty to start contributing or the lack of reward for very active contributors who aren\u0026rsquo;t core yet. Once a problem is resolved: discuss with your team about how we can avoid the same situation in the future. Make a retrospective if needed but talk and document the output.\nI continuously encourage at welcoming all kind of changes in TripleO so we can adopt new technologies that will make our project better.\nKeep in mind it has a cost. Some people will disagree but that\u0026rsquo;s fine: you might have to pick a rate of acceptance to consider that your team is ready to make this change.\nDelegate We are humans and have limits. We can\u0026rsquo;t be everywhere and do everything. We have to accept that PTLs are not supposed to be online 24/7. They don\u0026rsquo;t always have the best ideas and don\u0026rsquo;t always take the right decisions. This is fine. Your project will survive.\nI learnt that when I started to be PTL of TripleO in 2016. The TripleO team has become so big that I didn\u0026rsquo;t realize how many interruptions I would have every day. So I decided to learn how to delegate. We worked together and created TripleO Squads where each squad focus on a specific area of TripleO. Each squad would be autonomous enough to propose their own core reviewers or do their own meetings when needed. I wanted small teams working together, failing fast and making quick iterations so we could scale the project, accept and share the work load and increase the trust inside the TripleO team.\nThis is where I started to be a Consciously Competent PTL.\nWhere am I now I have reached a point where I think that projects wouldn\u0026rsquo;t need a PTL to run fine if they really wanted. Instead, I start to believe about some essential things that would actually help to get rid of this role:\nAs a team, define the vision of the project and document it. It will really help to know where we want to go and clear all expectations about the project. Establish trust to each individual by default and welcome newcomers. Encourage collective and distributed leadership. Try, Do, Fail, Learn, Teach. and start again. Don\u0026rsquo;t stale. This long journey helped me to learn many things in both technical and human areas. It has been awesome to work with such groups so far. I would like to spend more time on technical work (aka coding) but also in teaching and mentoring new contributors in OpenStack. Therefore, I won\u0026rsquo;t be PTL during the next cycle and my hope is to see new leaders in TripleO, who would come up with fresh ideas and help us to keep TripleO rocking.\nThanks for reading so far, and also thanks for your trust.\n","permalink":"https://my1.fr/blog/my-journey-as-an-openstack-ptl/","summary":"\u003cp\u003eThis story explains why I started to stop working as a anarchistic-multi-tasking-schedule-driven and learnt how to become a good team leader.\u003c/p\u003e","title":"My Journey As An OpenStack PTL"},{"content":" I don\u0026rsquo;t post much about my personal life, but I\u0026rsquo;m a dual-citizen (french and canadian) living in Quebec-City. Outside work, I love outdoors, spending time in family, flying aircrafts (private pilot) and a bunch of other things. My professional path is frequently updated on my Linkedin profile. I sometimes post on Twitter. If you want to reach out, please send me an email. ","permalink":"https://my1.fr/about-me/","summary":" I don\u0026rsquo;t post much about my personal life, but I\u0026rsquo;m a dual-citizen (french and canadian) living in Quebec-City. Outside work, I love outdoors, spending time in family, flying aircrafts (private pilot) and a bunch of other things. My professional path is frequently updated on my Linkedin profile. I sometimes post on Twitter. If you want to reach out, please send me an email. ","title":"About me"}]